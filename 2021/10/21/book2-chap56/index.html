<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>读书笔记||神经网络与深度学习5、6章 &mdash; Bird Nest</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/collection.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/common.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/posts/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/mzlogin/rouge-themes@master/dist/github.css"><link rel="canonical" href="https://uangjw.github.io/2021/10/21/book2-chap56/"><link rel="alternate" type="application/atom+xml" title="Bird Nest" href="https://uangjw.github.io"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/favicon.ico"><meta property="og:title" content="读书笔记||神经网络与深度学习5、6章"><meta name="keywords" content="Deep Learning"><meta name="og:keywords" content="Deep Learning"><meta name="description" content="第五章 深度神经网络为何很难训练深度神经网络：更多的隐藏层深度网络中，不同的层学习的速度差异很大深度神经网络中使用基于梯度下降的学习方法本身存在着内在不稳定性。这种不稳定性使得先前或者后面的层的学习过程阻滞观察隐藏层之间学习速度的差异，有这样一个结果：前面隐藏层中的神经元学习速度要慢于后面的隐藏层——消失的梯度问题（vanishing gradient problem）；一个不好的解决方法又会导致前面的层中的梯度变得非常大——激增的梯度问题（exploding gradient problem）深度神经网络中的梯度是不稳定的，在前面的层中或消失或激增梯度不稳定性前面的层上的梯度是来自后面的层上项的乘积，当存在过多的层次时，就出现了内在本质上的不稳定场景。所以如果使用标准的基于梯度的学习算法，在网络中的不同层就会出现按照不同学习速度学习的情况总之“什么让训练深度网络非常困难”这个问题相当复杂，除了基于梯度的学习方法是不稳定的，激活函数的选择，权重初始化甚至是学习算法的实现方式也扮演了重要的角色。网络结构和其他超参数本身也是很重要的。第六章 深度学习本章主要的部分是介绍深度卷积网络卷积神经网络是一个设法利用空间结构的架构（在解决图像识别问题的角度上）卷积神经网络采用三种基本概念：局部感受野local receptive fields，共享权重shared weights，混合pooling在图像识别的场景下解释这三个概念：局部感受野：将输入像素（一幅图像）连接到一个隐藏神经元层，但并不把每个输入像素连接到每个隐藏神经元，只是把输入图像进行小的、局部区域的连接；确切来说，第一个隐藏层中的每个神经元会连接到一个输入神经元（像素）的小区域，这个区域就被称为隐藏神经元的局部感受野。它是输入像素上的一个小窗口，一个连接学习一个权重，而隐藏神经元同时也学习一个总的偏置。可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。共享权重和偏置：局部感受野在原图像中按照所使用的跨距移动，构建起隐藏层，其中每个隐藏神经元具有一个偏置和连接到它的局部感受野的逐像素权重，且这一隐藏层中所有神经元都使用相同的权重与偏置。输入层到隐藏层的映射被称为一个特征映射，定义特征映射的权重称为共享权重，偏置称为共享偏置。共享权重和偏置经常被称为一个卷积核或者滤波器。共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数。共享权重和偏置的使用意味着一个隐藏层中所有神经元检测完全相同的特征，只是在输入图像的不同位置；这能很好地适应图像的平移不变性。混合层：又名池化层（pooling layer），通常紧接着在卷积层之后使用。它要做的是简化从卷积层输出的信息。混合层取得从卷积层输出的每一个特征映射（指隐藏神经元从该层输出的激活值），并从它们准备一个凝缩的特征映射。混合层的每一个单元会用来概括前一层的某个区域的神经元，例如最大值混合（max-pooling）就将这一个区域中的最大激活值输出。另一个常用的混合方法是L2混合，它取区域中激活值的平方和的平方根。一个典型的图像识别卷积神经网络结构：一层输入神经元，这些神经元用于对图像的像素强度进行编码；一个卷积层，使用axa的局部感受野和b个特征映射；一个最大值混合层，应用于cxc区域，遍及b个特征映射。网络中最后连接的层是一个全连接层。这一层将最大值混合层的每一个神经元连接到每一个输出神经元。补充循环神经网络RNN参考：https://zybuluo.com/hanbingtao/note/541458对于某些任务，它们要求能够更好地处理序列的信息，即前面的输入和后面的输入是有关系的；RNN就是为了适应这样的任务提出的。基本循环神经网络的计算方法可以用下面的公式表示：该网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。式1是输出层的计算公式，输出层是一个全连接层，$V$是输出层的权重矩阵，$g$是激活函数。式2是隐藏层的计算公式，它是循环层。$U$是输入x的权重矩阵，$W$是上一次的值$s_{t-1}$作为这一次的输入的权重矩阵，$f$是激活函数。循环层与全连接层的区别就是循环层多了一个权重矩阵W，反复把式2代入式1就可以看到，循环神经网络的输出值$o_t$是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、……影响的，这就是为什么循环神经网络可以注意到前面任意多个输入值的原因。"><meta name="og:description" content="第五章 深度神经网络为何很难训练深度神经网络：更多的隐藏层深度网络中，不同的层学习的速度差异很大深度神经网络中使用基于梯度下降的学习方法本身存在着内在不稳定性。这种不稳定性使得先前或者后面的层的学习过程阻滞观察隐藏层之间学习速度的差异，有这样一个结果：前面隐藏层中的神经元学习速度要慢于后面的隐藏层——消失的梯度问题（vanishing gradient problem）；一个不好的解决方法又会导致前面的层中的梯度变得非常大——激增的梯度问题（exploding gradient problem）深度神经网络中的梯度是不稳定的，在前面的层中或消失或激增梯度不稳定性前面的层上的梯度是来自后面的层上项的乘积，当存在过多的层次时，就出现了内在本质上的不稳定场景。所以如果使用标准的基于梯度的学习算法，在网络中的不同层就会出现按照不同学习速度学习的情况总之“什么让训练深度网络非常困难”这个问题相当复杂，除了基于梯度的学习方法是不稳定的，激活函数的选择，权重初始化甚至是学习算法的实现方式也扮演了重要的角色。网络结构和其他超参数本身也是很重要的。第六章 深度学习本章主要的部分是介绍深度卷积网络卷积神经网络是一个设法利用空间结构的架构（在解决图像识别问题的角度上）卷积神经网络采用三种基本概念：局部感受野local receptive fields，共享权重shared weights，混合pooling在图像识别的场景下解释这三个概念：局部感受野：将输入像素（一幅图像）连接到一个隐藏神经元层，但并不把每个输入像素连接到每个隐藏神经元，只是把输入图像进行小的、局部区域的连接；确切来说，第一个隐藏层中的每个神经元会连接到一个输入神经元（像素）的小区域，这个区域就被称为隐藏神经元的局部感受野。它是输入像素上的一个小窗口，一个连接学习一个权重，而隐藏神经元同时也学习一个总的偏置。可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。共享权重和偏置：局部感受野在原图像中按照所使用的跨距移动，构建起隐藏层，其中每个隐藏神经元具有一个偏置和连接到它的局部感受野的逐像素权重，且这一隐藏层中所有神经元都使用相同的权重与偏置。输入层到隐藏层的映射被称为一个特征映射，定义特征映射的权重称为共享权重，偏置称为共享偏置。共享权重和偏置经常被称为一个卷积核或者滤波器。共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数。共享权重和偏置的使用意味着一个隐藏层中所有神经元检测完全相同的特征，只是在输入图像的不同位置；这能很好地适应图像的平移不变性。混合层：又名池化层（pooling layer），通常紧接着在卷积层之后使用。它要做的是简化从卷积层输出的信息。混合层取得从卷积层输出的每一个特征映射（指隐藏神经元从该层输出的激活值），并从它们准备一个凝缩的特征映射。混合层的每一个单元会用来概括前一层的某个区域的神经元，例如最大值混合（max-pooling）就将这一个区域中的最大激活值输出。另一个常用的混合方法是L2混合，它取区域中激活值的平方和的平方根。一个典型的图像识别卷积神经网络结构：一层输入神经元，这些神经元用于对图像的像素强度进行编码；一个卷积层，使用axa的局部感受野和b个特征映射；一个最大值混合层，应用于cxc区域，遍及b个特征映射。网络中最后连接的层是一个全连接层。这一层将最大值混合层的每一个神经元连接到每一个输出神经元。补充循环神经网络RNN参考：https://zybuluo.com/hanbingtao/note/541458对于某些任务，它们要求能够更好地处理序列的信息，即前面的输入和后面的输入是有关系的；RNN就是为了适应这样的任务提出的。基本循环神经网络的计算方法可以用下面的公式表示：该网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。式1是输出层的计算公式，输出层是一个全连接层，$V$是输出层的权重矩阵，$g$是激活函数。式2是隐藏层的计算公式，它是循环层。$U$是输入x的权重矩阵，$W$是上一次的值$s_{t-1}$作为这一次的输入的权重矩阵，$f$是激活函数。循环层与全连接层的区别就是循环层多了一个权重矩阵W，反复把式2代入式1就可以看到，循环神经网络的输出值$o_t$是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、……影响的，这就是为什么循环神经网络可以注意到前面任意多个输入值的原因。"><meta property="og:url" content="https://uangjw.github.io/2021/10/21/book2-chap56/"><meta property="og:site_name" content="Bird Nest"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2021-10-21"> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery-ui.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://uangjw.github.io/" title="Bird Nest"><span class="octicon octicon-mark-github"></span> Bird Nest</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://uangjw.github.io/" class=" site-header-nav-item" target="" title="Home">Home</a> <a href="https://uangjw.github.io/categories/" class=" site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://uangjw.github.io/about/" class=" site-header-nav-item" target="" title="About">About</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="读书笔记||神经网络与深度学习"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">读书笔记||神经网络与深度学习5、6章</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2021/10/21 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#Deep Learning" title="Deep Learning">Deep Learning</a> </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#BookReport" title="BookReport">BookReport</a> </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#Basics" title="Basics">Basics</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 1876 字，约 6 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h3 id="第五章-深度神经网络为何很难训练">第五章 深度神经网络为何很难训练</h3><p>深度神经网络：更多的隐藏层</p><p>深度网络中，不同的层学习的速度差异很大</p><p>深度神经网络中使用基于梯度下降的学习方法本身存在着内在不稳定性。这种不稳定性使得先前或者后面的层的学习过程阻滞</p><p>观察隐藏层之间学习速度的差异，有这样一个结果：前面隐藏层中的神经元学习速度要慢于后面的隐藏层——消失的梯度问题（vanishing gradient problem）；一个不好的解决方法又会导致前面的层中的梯度变得非常大——激增的梯度问题（exploding gradient problem）</p><p>深度神经网络中的梯度是不稳定的，在前面的层中或消失或激增</p><h4 id="梯度不稳定性">梯度不稳定性</h4><p>前面的层上的梯度是来自后面的层上项的乘积，当存在过多的层次时，就出现了内在本质上的不稳定场景。所以如果使用标准的基于梯度的学习算法，在网络中的不同层就会出现按照不同学习速度学习的情况</p><p>总之“什么让训练深度网络非常困难”这个问题相当复杂，除了基于梯度的学习方法是不稳定的，激活函数的选择，权重初始化甚至是学习算法的实现方式也扮演了重要的角色。网络结构和其他超参数本身也是很重要的。</p><h3 id="第六章-深度学习">第六章 深度学习</h3><p>本章主要的部分是介绍深度卷积网络</p><p>卷积神经网络是一个设法利用空间结构的架构（在解决图像识别问题的角度上）</p><p>卷积神经网络采用三种基本概念：局部感受野local receptive fields，共享权重shared weights，混合pooling</p><p>在图像识别的场景下解释这三个概念：</p><p><strong>局部感受野：</strong>将输入像素（一幅图像）连接到一个隐藏神经元层，但并不把每个输入像素连接到每个隐藏神经元，只是把输入图像进行小的、局部区域的连接；确切来说，第一个隐藏层中的每个神经元会连接到一个输入神经元（像素）的小区域，这个区域就被称为隐藏神经元的局部感受野。它是输入像素上的一个小窗口，一个连接学习一个权重，而隐藏神经元同时也学习一个总的偏置。可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。</p><p><strong>共享权重和偏置：</strong>局部感受野在原图像中按照所使用的跨距移动，构建起隐藏层，其中每个隐藏神经元具有一个偏置和连接到它的局部感受野的逐像素权重，且这一隐藏层中所有神经元都使用相同的权重与偏置。输入层到隐藏层的映射被称为一个<strong>特征映射</strong>，定义特征映射的权重称为共享权重，偏置称为共享偏置。共享权重和偏置经常被称为一个<strong>卷积核</strong>或者<strong>滤波器</strong>。共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数。</p><p>共享权重和偏置的使用意味着一个隐藏层中所有神经元检测完全相同的特征，只是在输入图像的不同位置；这能很好地适应图像的平移不变性。</p><p><strong>混合层：</strong>又名池化层（pooling layer），通常紧接着在卷积层之后使用。它要做的是简化从卷积层输出的信息。混合层取得从卷积层输出的每一个特征映射（指隐藏神经元从该层输出的激活值），并从它们准备一个凝缩的特征映射。混合层的每一个单元会用来概括前一层的某个区域的神经元，例如最大值混合（max-pooling）就将这一个区域中的最大激活值输出。</p><p>另一个常用的混合方法是L2混合，它取区域中激活值的平方和的平方根。</p><p>一个典型的图像识别卷积神经网络结构：一层输入神经元，这些神经元用于对图像的像素强度进行编码；一个卷积层，使用axa的局部感受野和b个特征映射；一个最大值混合层，应用于cxc区域，遍及b个特征映射。网络中最后连接的层是一个全连接层。这一层将最大值混合层的每一个神经元连接到每一个输出神经元。</p><h3 id="补充">补充</h3><h4 id="循环神经网络rnn">循环神经网络RNN</h4><p>参考：https://zybuluo.com/hanbingtao/note/541458</p><p>对于某些任务，它们要求能够更好地处理序列的信息，即前面的输入和后面的输入是有关系的；RNN就是为了适应这样的任务提出的。</p><p>基本循环神经网络的计算方法可以用下面的公式表示：</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/RNNeq.png" style="zoom:67%;" /></p><p>该网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。式1是输出层的计算公式，输出层是一个全连接层，$V$是输出层的权重矩阵，$g$是激活函数。式2是隐藏层的计算公式，它是循环层。$U$是输入x的权重矩阵，$W$是上一次的值$s_{t-1}$作为这一次的输入的权重矩阵，$f$是激活函数。</p><p>循环层与全连接层的区别就是循环层多了一个权重矩阵W，反复把式2代入式1就可以看到，循环神经网络的输出值$o_t$是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、……影响的，这就是为什么循环神经网络可以注意到前面任意多个输入值的原因。</p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://uangjw.github.io" target="_blank">Jingwen Huang</a></li><li>本文链接：<a href="https://uangjw.github.io/2021/10/21/book2-chap56/" target="_blank">https://uangjw.github.io/2021/10/21/book2-chap56/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://uangjw.github.io/assets/search_data.json', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2021 <span title="Jingwen Huang">Jingwen Huang</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/uangjw/uangjw.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://uangjw.github.io/" title="Home" target="">Home</a></li><li> <a href="https://uangjw.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://uangjw.github.io/about/" title="About" target="">About</a></li><li><a href="https://uangjw.github.io"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul><script async src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="mobile-hidden" style="margin-top:8px"> <span id="busuanzi_container_site_pv" style="display:none"> 本站访问量<span id="busuanzi_value_site_pv"></span>次 </span> <span id="busuanzi_container_site_uv" style="display:none"> / 本站访客数<span id="busuanzi_value_site_uv"></span>人 </span> <span id="busuanzi_container_page_pv" style="display:none"> / 本页访问量<span id="busuanzi_value_page_pv"></span>次 / 统计始于2021-03-10 </span></div></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script><div style="display:none"> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-80669434-1', 'auto'); ga('send', 'pageview'); </script></div></body></html>
