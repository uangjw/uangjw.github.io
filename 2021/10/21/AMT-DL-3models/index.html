<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>论文笔记||三个基于深度学习的音乐转录模型 &mdash; Bird Nest</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/collection.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/common.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/posts/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/mzlogin/rouge-themes@master/dist/github.css"><link rel="canonical" href="https://uangjw.github.io/2021/10/21/AMT-DL-3models/"><link rel="alternate" type="application/atom+xml" title="Bird Nest" href="https://uangjw.github.io"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/favicon.ico"><meta property="og:title" content="论文笔记||三个基于深度学习的音乐转录模型"><meta name="keywords" content="Deep Learning, AMT"><meta name="og:keywords" content="Deep Learning, AMT"><meta name="description" content="三个基于深度学习的音乐转录模型"><meta name="og:description" content="三个基于深度学习的音乐转录模型"><meta property="og:url" content="https://uangjw.github.io/2021/10/21/AMT-DL-3models/"><meta property="og:site_name" content="Bird Nest"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2021-10-21"> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery-ui.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://uangjw.github.io/" title="Bird Nest"><span class="octicon octicon-mark-github"></span> Bird Nest</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://uangjw.github.io/" class=" site-header-nav-item" target="" title="Home">Home</a> <a href="https://uangjw.github.io/categories/" class=" site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://uangjw.github.io/about/" class=" site-header-nav-item" target="" title="About">About</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="论文笔记||三个基于深度学习的"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">论文笔记||三个基于深度学习的音乐转录模型</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2021/10/21 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#Deep Learning" title="Deep Learning">Deep Learning</a> </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#PaperNotes" title="PaperNotes">PaperNotes</a> </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#AMT" title="AMT">AMT</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 2953 字，约 9 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h1 id="三个基于深度学习的音乐转录模型">三个基于深度学习的音乐转录模型</h1><p>粗略了解三个模型，主要领会一下深度学习解决AMT问题的基本思想；总的来说本文是以下三篇论文的简要阅读笔记</p><p>An End-to-End Neural Network for Polyphonic Piano Music Transcription（2016）</p><p>Onsets and Frames: Dual-Objective Piano Transcription（2018谷歌）</p><p>High-resolution Piano Transcription with Pedals by Regressing Onset and Offset Times（2020字节）</p><p>除了文章本身，还参考了一琳同学的周报和舒心师姐的周报</p><h2 id="an-end-to-end-neural-network-for-polyphonic-piano-music-transcription">An End-to-End Neural Network for Polyphonic Piano Music Transcription</h2><p>一个CNN与RNN结合的端到端的钢琴演奏音乐转录模型。模型分为两部分：声学模型（acoustic model）与音乐语言模型（music language model）；这个模型结构是与语音识别系统类同的。声学模型与音乐语言模型之间通过一个概率图模型组合起来，最后使用波束搜索算法来在模型的输出上得到推断结果。</p><p>本文总结了一些过去的AMT（自动音乐转录）模型，所提出的模型也是后续许多新模型的基础。</p><h3 id="声学模型">声学模型</h3><p>声学模型输入特征$x$，输出$P(y$|$x)$，其中$y$是一个高维向量，其分量分别对应于$x$在钢琴键的各音高上的可能性。经过一番讨论，最终文章选用的声学模型是一个卷积神经网络。选择CNN的原因主要是它在为输入音频的一个帧做出音高预测时，能够同时提取与这一帧相邻的多个帧的时域与频域特征，这与音乐的特性是相适应的。</p><h3 id="音乐语言模型">音乐语言模型</h3><p>音乐语言模型的输入是音符的时间序列$y$（$y_t$是一个高维二值向量，表示$t$时刻所有活跃的音高），输出是$y$的概率分布$P(y)$。对于多调音乐（polyphonic music），同时活跃的音高是具有高度的相关性的，它们可能组成一个和弦，或是满足某一调式。</p><p>序列$y$的每一个值都是一个能表示钢琴所有音高中哪一个正在活跃的高维向量，要为这样的序列生成概率分布，需要使用RNN与NADE结合的模型。RNN与NADE都可以为输入找到概率分布，但RNN是为一个序列寻找，NADE是为一组高维输入寻找。直接使用RNN来为高维向量序列推测概率分布，则向量的各分量之间依然是独立的，在AMT问题里就相当于独立地看待同一时刻下活跃的音符，这就没有将音律的问题利用起来；而NADE在推测概率分布时并不考虑输入的顺序，即它将输入看作是一个无序的组而不是有序的序列。两者结合的RNN-NADE模型就能够比较好地推测高维向量时间序列的分布，结合方式简单来说就是让一系列的NADE在RNN的约束下学习。</p><h3 id="hybrid-rnn">Hybrid RNN</h3><p>将声学模型和音乐语言模型用概率图模型组合起来，就得到本文真正要提出的Hybrid RNN模型。在独立性假设</p><p>$P(y_t$|$y^{t-1}_0, x^{t-1}_0)=P(y_t$|$y_0^{t-1})$</p><p>$P(x_t$|$y_0^t,x_0^{t-1})=P(x_t$|$y_t)$</p><p>下可以得到</p><p>$P(y$|$x)\propto P(y_0$|$x_0)\prod^T_{t=1}P(y_t$|$y_0^{t-1})P(y_t$|$x_t)$</p><p>其中$P(y_t$|$x_t)$即输入$x_t$时对输出的预测，通过声学模型得到；$P(y_t$|$y_0^{t-1})$即序列中考虑前后全部序列信息后对输出的预测，通过音乐语言模型得到。</p><h3 id="conclusions-and-future-work">Conclusions and Future Work</h3><p>作者提出的部分问题与未来改进方向：</p><ol><li>AMT问题的有标签数据集太少</li><li>输入目前用的是CQT，改用VQT等具有更高时间分辨率的输入表示可能有帮助（2020年有个工作Automatic Music Transcription for Two Instruments based Variable Q-Transform and Deep Learning methods）</li><li>音乐语言模型里对调式的考虑还不够，可以对模型使用变调的输入来训练，或者是针对不同调式训练不同的模型</li><li>音乐语言模型的网络结构还可以改进，比如把输入层改成卷积层</li></ol><h2 id="onsets-and-frames-dual-objective-piano-transcription">Onsets and Frames: Dual-Objective Piano Transcription</h2><p>这是一个利用深度卷积与循环神经网络赖完成多调的钢琴演奏转录工作的项目。这一模型的特别之处在于，它预测音高的开始事件（onset），然后利用这些预测来训练音高的逐帧识别；具体来说，就是在一个帧里，帧识别器不能推理一个音符的开始，除非onset检测器认可这一帧中存在onset。在训练模型的过程中，作者注意同时加强onset以及偏移量（指音符持续量）的检测效果。</p><p>需要注意这里是对钢琴音符做起始位置识别；之所以能够这样做来提高AMT模型准确率，很大程度上还是和钢琴音符本身的特点有关系。钢琴音符的起始位置恰好就是音符振幅的峰值，且具有标志性的宽带频谱，所以识别钢琴演奏中的音符起始位置比较简单；同时钢琴敲键发声的原理也使得模型能够根据音符起始事件发生的速度来推断音符的音量，敲键快通常意味着这一音符的音量大，利用这一点转录生成的音频能够更好地记录钢琴演奏中的强度变化，结果更自然有感情（实际上演奏强度本来就是钢琴谱的一部分，如果说AMT的直接目的就是生成琴谱，那演奏强度本来就应该记录下来）。</p><h3 id="网络框架">网络框架</h3><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/google_AMT_model.png" style="zoom:67%;" /></p><p>模型的输入为229 mel-bins，2048 FFT Window size，16000Hz采样率的频谱结果。</p><p>onset检测器：卷积层的输出作为128 units双向LSTM的输入，LSTM后接一个88维输出的全连接层，88维的输出表示88个钢琴音高的onset概率</p><p>framewise检测器：卷积层接一个88维输出的全连接层，88维的输出向量和onset检测器端的对88个音高的onset预测向量级联成一个176维的向量，再过一个88维输出的全连接层。</p><p>LSTM（long short term网络）是一种特殊的RNN，它和简单RNN相比能够学习到连接更远的信息；双向的RNN是由两个RNN上下叠加在一起组成的，对于每个时刻t，输入会同时提供给两个方向相反的RNN，输出由两个RNN的状态共同决定，双向LSTM也是一样的。</p><h3 id="损失函数">损失函数</h3><p>好复杂，没懂，待补充……</p><h2 id="high-resolution-piano-transcription-with-pedals-by-regressing-onset-and-offset-times">High-resolution Piano Transcription with Pedals by Regressing Onset and Offset Times</h2><p>如题目所说，与谷歌的项目相比提高了onset检测的时间分辨率（从帧到毫秒），且增加了对延音踏板的转录。</p><p>为什么要提高onset检测的分辨率？以谷歌的模型为例，它只找到并标出onset事件发生的帧，不能给出具体发生的时刻，进而不能给出音符attack的持续时间；另一方面，很自然的，对于逐帧的转录模型的转录结果，其分辨率会受帧长选择的影响，为了提高结果的时间分辨率而一味缩短帧长又会提高计算开销。</p><p>本文提高检测分辨率的方式是，定义每帧的target值为此帧的中间时刻到最近的onset时刻的时间。</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/HigherOnsetResolution.png" style="zoom:67%;" /></p><h3 id="网络框架-1">网络框架</h3><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/ByteDance_AMT_model.png" style="zoom:67%;" /></p><p>还看不太懂，待补充……</p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://uangjw.github.io" target="_blank">Jingwen Huang</a></li><li>本文链接：<a href="https://uangjw.github.io/2021/10/21/AMT-DL-3models/" target="_blank">https://uangjw.github.io/2021/10/21/AMT-DL-3models/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://uangjw.github.io/assets/search_data.json', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2021 <span title="Jingwen Huang">Jingwen Huang</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/uangjw/uangjw.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://uangjw.github.io/" title="Home" target="">Home</a></li><li> <a href="https://uangjw.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://uangjw.github.io/about/" title="About" target="">About</a></li><li><a href="https://uangjw.github.io"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul><script async src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="mobile-hidden" style="margin-top:8px"> <span id="busuanzi_container_site_pv" style="display:none"> 本站访问量<span id="busuanzi_value_site_pv"></span>次 </span> <span id="busuanzi_container_site_uv" style="display:none"> / 本站访客数<span id="busuanzi_value_site_uv"></span>人 </span> <span id="busuanzi_container_page_pv" style="display:none"> / 本页访问量<span id="busuanzi_value_page_pv"></span>次 / 统计始于2021-03-10 </span></div></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script><div style="display:none"> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-80669434-1', 'auto'); ga('send', 'pageview'); </script></div></body></html>
