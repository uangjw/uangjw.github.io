<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>不基于模型的预测与控制 &mdash; Bird Nest</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/collection.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/common.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/posts/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/mzlogin/rouge-themes@master/dist/github.css"><link rel="canonical" href="https://uangjw.github.io/2021/10/24/RL-model-free-control/"><link rel="alternate" type="application/atom+xml" title="Bird Nest" href="https://uangjw.github.io"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/favicon.ico"><meta property="og:title" content="不基于模型的预测与控制"><meta name="keywords" content="Reinforce Learning, MDP"><meta name="og:keywords" content="Reinforce Learning, MDP"><meta name="description" content="主要参考陈旭老师的ppt以及知乎专栏https://www.zhihu.com/column/reinforce"><meta name="og:description" content="主要参考陈旭老师的ppt以及知乎专栏https://www.zhihu.com/column/reinforce"><meta property="og:url" content="https://uangjw.github.io/2021/10/24/RL-model-free-control/"><meta property="og:site_name" content="Bird Nest"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2021-10-24"> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery-ui.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://uangjw.github.io/" title="Bird Nest"><span class="octicon octicon-mark-github"></span> Bird Nest</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://uangjw.github.io/" class=" site-header-nav-item" target="" title="Home">Home</a> <a href="https://uangjw.github.io/categories/" class=" site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://uangjw.github.io/about/" class=" site-header-nav-item" target="" title="About">About</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="不基于模型的预测与控制"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">不基于模型的预测与控制</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2021/10/24 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#Reinforcement Learning" title="Reinforcement Learning">Reinforcement Learning</a> </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#Basics" title="Basics">Basics</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 3526 字，约 11 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><p>主要参考陈旭老师的ppt以及知乎专栏https://www.zhihu.com/column/reinforce</p><p>MDP未知具体来说就是$R(s,a)$与$P(s’$|$s,a)$是未知的；在现实生活中，MDP模型要么是未知的，要么是已知但实在是太大太复杂了。所以不基于模型的预测与控制方法是必要的。</p><p>Model-free RL是通过与环境交互来学习的。在这里需要引入一个episode（又称trajectory，老师称之为“轨迹”）的概念，episode是agent通过与环境交互得到的一个集合，其内容可以表示为</p><p>${S_1, A_1, R_1, S_2, A_2, R_2, … S_T, A_T, R_T}$</p><h3 id="model-free-prediction">Model-free Prediction</h3><p>这里不基于模型的预测指在一个未知的MDP上估计一个policy的价值。课堂介绍了两种方法：</p><h4 id="monte-carlo-policy-evaluation">Monte Carlo policy evaluation</h4><p>蒙特卡洛策略评估的目标是，在给定的policy下，从一系列完整的episode中学习得到该policy下的状态价值函数。其特点是使用有限的完整episode产生的经验性信息推导出每个状态的平均收获，以此来替代收获的期望即状态价值。</p><p>基于特定policy $\pi$的一个episode信息可以表示为如下的序列</p><p>${S_1, A_1, R_2, S_2, A_2, R_3, …, S_t, A_t, R_{t+1},…,S_k}$ ~ $\pi$</p><p>注意这里$R$的下标与前面介绍episode的下标不一样，这里$R_{t+1}$表示的是$t$时刻个体在状态$S_t$获得的即时奖励。更准确地说是个体在状态$S_t$执行一个行为$a$后离开该状态获得的即时奖励。</p><p>$t$时刻状态$S_t$的收获：</p><p>$G_t=R_{t+1}+\gamma R_{t+2}+…+\gamma^{T-1}R_T$</p><p>其中$T$为终止时刻。于是该policy下某一状态$s$的价值就可表示为</p><p>$v_{\pi}(s)=E_{\pi}[G_t$|$S_t=s]$</p><p><strong>Incremental MC Updates</strong></p><p>在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态。一个在实际操作中常用的方法就是把Incremental Mean方法应用于蒙特卡洛策略评估，得到蒙特卡洛累进更新方法。</p><p>对于一系列episode中的每一个episode，对这个episode里的每一个状态$S_t$，有一个收获$G_t$，每碰到一次$S_t$，使用下式计算状态的平均价值$V(S_t)$：</p><p>$V(S_t)\leftarrow V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))$</p><p>其中$N(S_t)$就是遇到这个状态的次数，整个episode信息分析完毕后$V(S_t)$就是所有$S_t$的收获的平均值。在处理非静态问题时，使用这个方法跟踪一个实时更新的平均值是非常有用的，可以扔掉那些已经计算过的episode信息。此时可以引入参数$\alpha$来更新状态价值：</p><p>$V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))$</p><h4 id="temporal-difference-td-learning">Temporal Difference (TD) learning</h4><p>MC方法其实有很多缺点，实际应用并不多；时序差分（TD）学习方法才是实际常用的。TD学习也从episode学习，但它可以学习不完整的episode，通过自身的bootstrap来猜测episode的结果，同时持续更新这个猜测。</p><p>TD学习中算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励$R_{t+1}$与下一状态$S_{t+1}$的预估状态价值乘以衰减系数$\gamma$组成，符合Bellman方程的描述：</p><p>$V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))$</p><p>式中：</p><p>$R_{t+1}+\gamma V(S_{t+1})$称为TD目标值</p><p>$\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$称为TD误差</p><p>bootstrapping指的就是TD目标值代替收获$G_t$的过程。可以理解为“引导”。</p><h4 id="mc对比td">MC对比TD</h4><p>一个episode中，TD在知道结果之前可以学习，MC必须等到最后结果才能学习；也就是说，TD可以在持续进行的没有结果的环境中学习。</p><p>MC使用的$G_t$是实际收获，是基于某一策略状态价值的无偏估计；而TD target是基于下一状态的预估价值计算的当前预估收获，是当前状态实际价值的有偏估计。如果用的是下一状态的实际价值来计算当前状态，那就是true TD target，是对当前状态实际价值的无偏估计：</p><p>$R_{t+1}+\gamma v_{\pi}(S_{t+1})$</p><p>TD算法使用了MDP问题的马尔可夫属性，在Markov环境下更有效；但MC算法并不利用马尔可夫属性，通常在非Markov环境下更有效。</p><h4 id="mctddp">MC/TD/DP</h4><p>MC与TD与DP都是计算MDP模型的状态价值的方法。前两种是在模型未知的情况下的常用方法，MC要完整的episode来更新状态价值，TD不需要完整的episode；DP方法是在模型已知时使用的计算状态价值的方法，通过计算一个状态$S$所有可能的转移状态$S’$及其转移概率以及对应的即时奖励来计算这个状态$S$的价值。</p><p>MC没有引导数据，只使用实际收获；DP和TD都有引导数据。</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/DP_view.png" style="zoom:67%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/MC_view.png" style="zoom:67%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/TD_view.png" style="zoom:67%;" /></p><h3 id="model-free-control">Model-free control</h3><p>这里不基于模型的控制指为一个未知的MDP最优化其价值函数。</p><h4 id="policy-iteration">Policy Iteration</h4><p>通用策略迭代的核心是在两个交替的过程之间进行策略优化。一个过程是策略评估，另一个是改善策略。从一个策略$\pi$和一个价值函数$V$开始，每一次箭头向上代表着利用当前策略进行价值函数的更新，每一次箭头向下代表着根据更新的价值函数贪婪地选择新的策略，说它是贪婪的，是因为每次都采取转移到可能的、状态函数最高的新状态的行为。最终将收敛至最优策略和最优价值函数。</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/policy_iter.png" style="zoom:67%;" /></p><p>这种方法不适用于模型未知的蒙特卡洛学习。我们需要使用状态行为对下的价值$Q(s,a)$来代替状态价值，这样做的目的是可以改善策略而不用知道整个模型，只需要知道在某个状态下采取什么样的行为价值最大即可。具体是这样：从一个初始的$Q$和策略$\pi$开始，先根据这个策略更新每一个状态行为对的$q$值，$s$随后基于更新的$Q$确定改善的贪婪算法。</p><p>但如果我们每次都贪婪地改善策略，很有可能会导致采样经验不足，从而产生一个并不是最优的策略。我们需要不时地尝试一些新的行为，$\epsilon$-exploration就是一个折中的探索方案：</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/epsilon_explore.png" style="zoom:67%;" /></p><h4 id="sarsa-on-policy-td-control">SARSA: On-policy TD Control</h4><p>SARSA算法是一种在控制问题中使用TD学习方法来获得状态行为价值$Q$的估计的算法。</p><p>SARSA是on-policy的：个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略（一句话就是要优化的策略就是当前遵循的策略）。</p><p>伪代码：</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/SARSA.png" style="zoom:67%;" /></p><p>在算法中，$Q(s,a)$是以一张大表存储的，这不适用于解决规模很大的问题。</p><h4 id="q-learning-off-policy-td-control">Q-learning: Off-policy TD Control</h4><p>off-policy与on-policy不同，off-policy的学习方法运行在两个policy上，一个target policy，一个behavior policy。off-policy学习是从behavior policy上采样得到经验从而改进target policy的，换句话说就是采用一个现有的policy，遵从它来与环境交互，获得数据，再利用这些数据来评估另一个policy。这种方法的好处能够在实际应用中体现出来，一个是能够从人类或其他智能体的经验中学习，另一个是能够从旧有的policy中学习。</p><p>Q-learning是一种基于TD的off-policy的控制方法，要点在于：更新一个状态行为价值$Q$时，采用的不是当前遵循策略的下一个状态行为价值$Q’$，而是采用待评估策略产生的下一个状态行为价值$Q’$。公式：</p><p>$Q(S_t,A_t)\leftarrow Q(S_t, A_t)+\alpha(R_{t+1}+\gamma Q(S_{t+1}, A’)-Q(S_t, A_t))$</p><p>其中TD target是基于待评估策略产生的行为$A’$得到的$Q$价值。</p><p>伪代码：</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/Q_learning.png" style="zoom:67%;" /></p><p>上面的伪代码展示的是一种，将完全greedy地改进policy（下一状态选择最大Q对应的a作为行动）作为target policy的Q-learning</p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://uangjw.github.io" target="_blank">Jingwen Huang</a></li><li>本文链接：<a href="https://uangjw.github.io/2021/10/24/RL-model-free-control/" target="_blank">https://uangjw.github.io/2021/10/24/RL-model-free-control/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://uangjw.github.io/assets/search_data.json', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2021 <span title="Jingwen Huang">Jingwen Huang</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/uangjw/uangjw.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://uangjw.github.io/" title="Home" target="">Home</a></li><li> <a href="https://uangjw.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://uangjw.github.io/about/" title="About" target="">About</a></li><li><a href="https://uangjw.github.io"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul><script async src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="mobile-hidden" style="margin-top:8px"> <span id="busuanzi_container_site_pv" style="display:none"> 本站访问量<span id="busuanzi_value_site_pv"></span>次 </span> <span id="busuanzi_container_site_uv" style="display:none"> / 本站访客数<span id="busuanzi_value_site_uv"></span>人 </span> <span id="busuanzi_container_page_pv" style="display:none"> / 本页访问量<span id="busuanzi_value_page_pv"></span>次 / 统计始于2021-03-10 </span></div></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script><div style="display:none"> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-80669434-1', 'auto'); ga('send', 'pageview'); </script></div></body></html>
