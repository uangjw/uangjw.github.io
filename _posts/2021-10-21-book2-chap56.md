---
layout: post
title: 读书笔记||神经网络与深度学习5、6章
categories: [Deep Learning, BookReport, Basics]
description: Deep Learning
keywords: Deep Learning

---

​	*The Algorithms and Principles of Non-photorealistic Graphics*（下称书） 第一章主要介绍了真实感渲染与非真实感渲染的发展过程与区别、非真实感渲染的概念框架以及研究方向等。

1. 真实感渲染与非真实感渲染的区别

   - 渲染的内容不同。真实感渲染关注场景的立体几何与形状等特征，期待得到的渲染结果能够忠实反映真实场景，也即希望能够从某种程度上“客观重现”场景（完全符合物理规律几何规律）。而非真实感渲染是“主观的”，所生成的画面是显然不存在于真实世界的。
   - 表达方式不同。真实感渲染正如第一点所说，生成的是完全与真实世界无二的画面。非真实感渲染由于并不追求真实感，因而能够在结果中改变对原物体的某种特征或画面的某些关联的强调程度，为创造性与想象力提供了相当的空间。
   - 观者对结果图像的认知感受不同。真实感渲染生成的图像更大程度上是一种“直觉”，而要解读非真实感渲染的图像则需要一个理解的过程。这一点和前面提到的非真实感渲染的“主观性”是一致的。
   - 算法机制不同。真实感渲染技术试图模拟相机的工作，逐像素地进行渲染的过程正如光线传播的物理现实。非真实感渲染所采取的方式是逐区域的。概括来说，这些区域实际上就是“画笔与画布进行交互的区域”。
   - 真实感渲染是一个从3D场景到2D画面的单向的投影。但非真实感渲染是3D模型与2D图像之间复杂的双向过程。或许可以理解为，真实感渲染只需对输入的3D场景数据（几何描述、材质、光源等）进行“一次性的”处理，即模拟光线等，就可得到符合预期的接近现实场景的结果（如果中间的处理过程没有问题）；但非真实感渲染受其“主观性”影响，用户往往会对生成的2D图像进行评估，然后再次调整某些参数来由3D模型重新生成图像，如此不断反复以获得满意的结果。

2. 非真实感渲染的主要框架

   非真实感渲染的传统方向即是生成类似名画家作品的图片。这里的问题是艺术创作本身就是讲究“创意”或“灵感”的过程，要想通过算法来描述难度较大。另一个方向则是基于认知心理学来对可视化问题做优化。下面是书本提供的非真实感渲染领域较为核心的几个问题。

   - 如何在空白画布上创造绘画（或其他形式的艺术）。仿照一般艺术创作的过程，计算机需要从颜料（媒介）、画布（某种表面）、画笔（能够把媒介“画”在表面上）三个方面来进行建模与模拟。用户通过这些数字化的工具能够与计算机交互着创作，或是指导计算机半自动地生成作品。
   - 如何将源图片转换为具有期望视觉效果的画面。可能的方式有设置参数、文本关键字或样本等。
   - 如何从3D模型生成艺术化的渲染结果。这里注意区分与真实感渲染的不同。虽同为对3D模型进行渲染，但此处提到的渲染结果是更具非真实感的，并且能够根据用户需求改变具体艺术风格（不必忠于客观现实）。
   - 如何从纹理、图形等数据生成“有意义”“可被理解”的图片。
   - 如何加速具有时间连贯性的卡通动画序列的制作。这一点我在理解时遇到了一些困难。现在我的理解大概是：2D动画的制作中，动画师是主体而计算机只起辅助作用。而3D动画的制作则更多的是计算机的工作。一个原因是，3D动画中建模、渲染和动作是可以各自独立完成的，但在2D动画中它们隐式地高度耦合。假如试图在2D动画的制作过程中将这三个过程分开进行，则难以避免的时间上的“错位”将引起画面的混乱，影响视觉效果。


​《神经网络与深度学习》第五章、第六章粗略阅读笔记

### 第五章 深度神经网络为何很难训练

深度神经网络：更多的隐藏层 

深度网络中，不同的层学习的速度差异很大

深度神经网络中使用基于梯度下降的学习方法本身存在着内在不稳定性。这种不稳定性使得先前或者后面的层的学习过程阻滞

观察隐藏层之间学习速度的差异，有这样一个结果：前面隐藏层中的神经元学习速度要慢于后面的隐藏层——消失的梯度问题（vanishing gradient problem）；一个不好的解决方法又会导致前面的层中的梯度变得非常大——激增的梯度问题（exploding gradient problem）

深度神经网络中的梯度是不稳定的，在前面的层中或消失或激增

#### 梯度不稳定性

前面的层上的梯度是来自后面的层上项的乘积，当存在过多的层次时，就出现了内在本质上的不稳定场景。所以如果使用标准的基于梯度的学习算法，在网络中的不同层就会出现按照不同学习速度学习的情况

总之“什么让训练深度网络非常困难”这个问题相当复杂，除了基于梯度的学习方法是不稳定的，激活函数的选择，权重初始化甚至是学习算法的实现方式也扮演了重要的角色。网络结构和其他超参数本身也是很重要的。

### 第六章 深度学习

本章主要的部分是介绍深度卷积网络

卷积神经网络是一个设法利用空间结构的架构（在解决图像识别问题的角度上）

卷积神经网络采用三种基本概念：局部感受野local receptive fields，共享权重shared weights，混合pooling

在图像识别的场景下解释这三个概念：

**局部感受野：**将输入像素（一幅图像）连接到一个隐藏神经元层，但并不把每个输入像素连接到每个隐藏神经元，只是把输入图像进行小的、局部区域的连接；确切来说，第一个隐藏层中的每个神经元会连接到一个输入神经元（像素）的小区域，这个区域就被称为隐藏神经元的局部感受野。它是输入像素上的一个小窗口，一个连接学习一个权重，而隐藏神经元同时也学习一个总的偏置。可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。

**共享权重和偏置：**局部感受野在原图像中按照所使用的跨距移动，构建起隐藏层，其中每个隐藏神经元具有一个偏置和连接到它的局部感受野的逐像素权重，且这一隐藏层中所有神经元都使用相同的权重与偏置。输入层到隐藏层的映射被称为一个**特征映射**，定义特征映射的权重称为共享权重，偏置称为共享偏置。共享权重和偏置经常被称为一个**卷积核**或者**滤波器**。共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数。

共享权重和偏置的使用意味着一个隐藏层中所有神经元检测完全相同的特征，只是在输入图像的不同位置；这能很好地适应图像的平移不变性。

**混合层：**又名池化层（pooling layer），通常紧接着在卷积层之后使用。它要做的是简化从卷积层输出的信息。混合层取得从卷积层输出的每一个特征映射（指隐藏神经元从该层输出的激活值），并从它们准备一个凝缩的特征映射。混合层的每一个单元会用来概括前一层的某个区域的神经元，例如最大值混合（max-pooling）就将这一个区域中的最大激活值输出。

另一个常用的混合方法是L2混合，它取区域中激活值的平方和的平方根。

一个典型的图像识别卷积神经网络结构：一层输入神经元，这些神经元用于对图像的像素强度进行编码；一个卷积层，使用axa的局部感受野和b个特征映射；一个最大值混合层，应用于cxc区域，遍及b个特征映射。网络中最后连接的层是一个全连接层。这一层将最大值混合层的每一个神经元连接到每一个输出神经元。

### 补充

#### 循环神经网络RNN

参考：https://zybuluo.com/hanbingtao/note/541458

对于某些任务，它们要求能够更好地处理序列的信息，即前面的输入和后面的输入是有关系的；RNN就是为了适应这样的任务提出的。

基本循环神经网络的计算方法可以用下面的公式表示：

<img src="/images/RNNeq.png" style="zoom:67%;" />

该网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。式1是输出层的计算公式，输出层是一个全连接层，$V$是输出层的权重矩阵，$g$是激活函数。式2是隐藏层的计算公式，它是循环层。$U$是输入x的权重矩阵，$W$是上一次的值$s_{t-1}$作为这一次的输入的权重矩阵，$f$是激活函数。

循环层与全连接层的区别就是循环层多了一个权重矩阵W，反复把式2代入式1就可以看到，循环神经网络的输出值$o_t$是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、……影响的，这就是为什么循环神经网络可以注意到前面任意多个输入值的原因。

